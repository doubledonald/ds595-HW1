{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Sampler Synthesis — Starter Code\n",
    "\n",
    "This notebook provides baseline implementations of Random Walk Metropolis-Hastings and HMC using [BlackJAX](https://blackjax-devs.github.io/blackjax/). Use these as reference points for your novel sampler.\n",
    "\n",
    "**Your task**: Design, implement, and analyze a novel MCMC sampler. Compare it to these baselines on the benchmark distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import blackjax\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 4)\n",
    "plt.rcParams[\"font.size\"] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Benchmark Distribution: Rosenbrock (Banana)\n",
    "\n",
    "The Rosenbrock distribution has a curved, narrow ridge that tests how well samplers handle strong correlations and curved geometry. This is challenging because:\n",
    "- The high-probability region is thin and curved\n",
    "- Random walk proposals often step off the ridge\n",
    "- Samplers need to follow the curved geometry efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prob_rosenbrock(theta):\n",
    "    \"\"\"Rosenbrock (banana) distribution.\n",
    "\n",
    "    log p(x, y) ∝ -(1-x)²/20 - (y - x²)²\n",
    "\n",
    "    This creates a curved, banana-shaped distribution that tests\n",
    "    how well samplers handle strong correlations and curved geometry.\n",
    "    \"\"\"\n",
    "    x, y = theta[0], theta[1]\n",
    "    return -0.05 * (1 - x) ** 2 - (y - x**2) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution(log_prob_fn, title, xlim=(-4, 4), ylim=(-4, 4)):\n",
    "    \"\"\"Visualize a 2D log probability distribution.\"\"\"\n",
    "    x = jnp.linspace(*xlim, 200)\n",
    "    y = jnp.linspace(*ylim, 200)\n",
    "    X, Y = jnp.meshgrid(x, y)\n",
    "    positions = jnp.stack([X.ravel(), Y.ravel()], axis=-1)\n",
    "\n",
    "    log_probs = jax.vmap(log_prob_fn)(positions).reshape(X.shape)\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.contourf(X, Y, jnp.exp(log_probs), levels=50, cmap=\"viridis\")\n",
    "    plt.colorbar(label=\"Probability density\")\n",
    "    plt.xlabel(r\"$x$\")\n",
    "    plt.ylabel(r\"$y$\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_distribution(log_prob_rosenbrock, \"Rosenbrock (Banana)\", xlim=(-2, 3), ylim=(-1, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Baseline 1: Random Walk Metropolis-Hastings\n",
    "\n",
    "The simplest MCMC method. Proposes isotropic Gaussian steps — no gradient information.\n",
    "\n",
    "**Tuning tip:** Target ~23-50% acceptance rate. Higher isn't better — it means steps are too small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rwmh(key, log_prob_fn, initial_position, sigma, n_samples):\n",
    "    \"\"\"Run Random Walk Metropolis-Hastings using BlackJAX.\n",
    "\n",
    "    Args:\n",
    "        key: JAX random key\n",
    "        log_prob_fn: Log probability function\n",
    "        initial_position: Starting point, shape (D,)\n",
    "        sigma: Proposal standard deviation (scalar or array)\n",
    "        n_samples: Number of samples to draw\n",
    "\n",
    "    Returns:\n",
    "        samples: Array of shape (n_samples, D)\n",
    "        acceptance_rate: Fraction of accepted proposals\n",
    "    \"\"\"\n",
    "    # Initialize the sampler with a normal proposal distribution\n",
    "    rmh = blackjax.rmh(log_prob_fn, blackjax.mcmc.random_walk.normal(sigma))\n",
    "    initial_state = rmh.init(initial_position)\n",
    "\n",
    "    # Build the sampling loop\n",
    "    @jax.jit\n",
    "    def one_step(state, key):\n",
    "        state, info = rmh.step(key, state)\n",
    "        return state, (state.position, info.is_accepted)\n",
    "\n",
    "    # Run the chain\n",
    "    keys = jr.split(key, n_samples)\n",
    "    _, (samples, accepted) = jax.lax.scan(one_step, initial_state, keys)\n",
    "\n",
    "    return samples, accepted.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Baseline 2: Hamiltonian Monte Carlo (HMC)\n",
    "\n",
    "Uses gradient information to make informed proposals. Typically much more efficient than random walk.\n",
    "\n",
    "**Tuning tip:** Target ~65-90% acceptance rate. Tune step_size first, then n_leapfrog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_hmc(key, log_prob_fn, initial_position, step_size, n_leapfrog, n_samples):\n",
    "    \"\"\"Run HMC using BlackJAX.\n",
    "\n",
    "    Args:\n",
    "        key: JAX random key\n",
    "        log_prob_fn: Log probability function\n",
    "        initial_position: Starting point, shape (D,)\n",
    "        step_size: Leapfrog step size (epsilon)\n",
    "        n_leapfrog: Number of leapfrog steps per iteration\n",
    "        n_samples: Number of samples to draw\n",
    "\n",
    "    Returns:\n",
    "        samples: Array of shape (n_samples, D)\n",
    "        acceptance_rate: Fraction of accepted proposals\n",
    "    \"\"\"\n",
    "    # Initialize the sampler (identity mass matrix)\n",
    "    inverse_mass_matrix = jnp.ones(initial_position.shape[0])\n",
    "    hmc = blackjax.hmc(\n",
    "        log_prob_fn,\n",
    "        step_size=step_size,\n",
    "        inverse_mass_matrix=inverse_mass_matrix,\n",
    "        num_integration_steps=n_leapfrog,\n",
    "    )\n",
    "    initial_state = hmc.init(initial_position)\n",
    "\n",
    "    # Build the sampling loop\n",
    "    @jax.jit\n",
    "    def one_step(state, key):\n",
    "        state, info = hmc.step(key, state)\n",
    "        return state, (state.position, info.acceptance_rate)\n",
    "\n",
    "    # Run the chain\n",
    "    keys = jr.split(key, n_samples)\n",
    "    _, (samples, accepted) = jax.lax.scan(one_step, initial_state, keys)\n",
    "\n",
    "    return samples, accepted.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Run Baselines on Rosenbrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jr.PRNGKey(42)\n",
    "key1, key2 = jr.split(key)\n",
    "\n",
    "initial_pos = jnp.array([0.0, 0.0])\n",
    "n_samples = 50_000\n",
    "\n",
    "# Random Walk MH\n",
    "rwmh_samples, rwmh_acc = run_rwmh(key1, log_prob_rosenbrock, initial_pos, sigma=1.0, n_samples=n_samples)\n",
    "print(f\"RWMH acceptance rate: {rwmh_acc:.2%}\")\n",
    "\n",
    "# HMC\n",
    "hmc_samples, hmc_acc = run_hmc(\n",
    "    key2, log_prob_rosenbrock, initial_pos, step_size=0.2, n_leapfrog=10, n_samples=n_samples\n",
    ")\n",
    "print(f\"HMC acceptance rate: {hmc_acc:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_samples_comparison(samples1, samples2, label1, label2, log_prob_fn, xlim, ylim):\n",
    "    \"\"\"Plot samples from two methods side by side.\"\"\"\n",
    "    # Compute contours\n",
    "    x = jnp.linspace(*xlim, 100)\n",
    "    y = jnp.linspace(*ylim, 100)\n",
    "    X, Y = jnp.meshgrid(x, y)\n",
    "    positions = jnp.stack([X.ravel(), Y.ravel()], axis=-1)\n",
    "    log_probs = jax.vmap(log_prob_fn)(positions).reshape(X.shape)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    for ax, samples, label in zip(axes, [samples1, samples2], [label1, label2]):\n",
    "        ax.contour(X, Y, jnp.exp(log_probs), levels=10, colors=\"gray\", alpha=0.5)\n",
    "        ax.scatter(samples[::5, 0], samples[::5, 1], alpha=0.3, s=5, c=\"blue\")\n",
    "        ax.set_xlabel(r\"$\\theta_0$\")\n",
    "        ax.set_ylabel(r\"$\\theta_1$\")\n",
    "        ax.set_title(label)\n",
    "        ax.set_xlim(xlim)\n",
    "        ax.set_ylim(ylim)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_samples_comparison(\n",
    "    rwmh_samples,\n",
    "    hmc_samples,\n",
    "    f\"RWMH (acc={rwmh_acc:.1%})\",\n",
    "    f\"HMC (acc={hmc_acc:.1%})\",\n",
    "    log_prob_rosenbrock,\n",
    "    xlim=(-2, 3),\n",
    "    ylim=(-1, 5),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Diagnostics with ArviZ\n",
    "\n",
    "[ArviZ](https://python.arviz.org/) provides standard MCMC diagnostics. Key metrics:\n",
    "- **Acceptance rate**: Too low = proposals too aggressive; too high = proposals too timid\n",
    "- **Effective Sample Size (ESS)**: How many independent samples you effectively have\n",
    "- **Trace plots**: Visual check for mixing and stationarity\n",
    "- **Autocorrelation**: How correlated successive samples are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def samples_to_inference_data(samples, var_names=None):\n",
    "    \"\"\"Convert samples array to ArviZ InferenceData.\n",
    "\n",
    "    Args:\n",
    "        samples: Array of shape (n_samples, n_dims)\n",
    "        var_names: Optional list of variable names\n",
    "\n",
    "    Returns:\n",
    "        ArviZ InferenceData object\n",
    "    \"\"\"\n",
    "    if var_names is None:\n",
    "        var_names = [f\"theta_{i}\" for i in range(samples.shape[1])]\n",
    "\n",
    "    # ArviZ expects dict of {var_name: array} with shape (n_chains, n_samples)\n",
    "    data_dict = {name: samples[None, :, i] for i, name in enumerate(var_names)}\n",
    "    return az.convert_to_inference_data(data_dict)\n",
    "\n",
    "\n",
    "def summarize_sampler(samples, name, var_names=None):\n",
    "    \"\"\"Print summary statistics for samples using ArviZ.\"\"\"\n",
    "    idata = samples_to_inference_data(samples, var_names)\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    display(az.summary(idata, kind=\"stats\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rosenbrock diagnostics\n",
    "var_names = [\"x\", \"y\"]\n",
    "\n",
    "rwmh_idata = samples_to_inference_data(rwmh_samples, var_names)\n",
    "hmc_idata = samples_to_inference_data(hmc_samples, var_names)\n",
    "\n",
    "# Summary statistics (mean, sd, ESS)\n",
    "summarize_sampler(rwmh_samples, \"RWMH — Rosenbrock\", var_names)\n",
    "summarize_sampler(hmc_samples, \"HMC — Rosenbrock\", var_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trace plots — Rosenbrock\n",
    "print(\"RWMH Trace Plots:\")\n",
    "az.plot_trace(rwmh_idata, combined=True, figsize=(12, 4))\n",
    "plt.suptitle(\"RWMH — Rosenbrock\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nHMC Trace Plots:\")\n",
    "az.plot_trace(hmc_idata, combined=True, figsize=(12, 4))\n",
    "plt.suptitle(\"HMC — Rosenbrock\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autocorrelation — Rosenbrock\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 3))\n",
    "az.plot_autocorr(rwmh_idata, ax=axes[0], combined=True)\n",
    "axes[0].set_title(\"RWMH — Autocorrelation\")\n",
    "az.plot_autocorr(hmc_idata, ax=axes[1], combined=True)\n",
    "axes[1].set_title(\"HMC — Autocorrelation\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Discussion: Why Does RWMH Struggle Here?\n",
    "\n",
    "Look carefully at the results above. You might notice something surprising: RWMH has *lower* autocorrelation than HMC, yet HMC explores the distribution much better. What's going on?\n",
    "\n",
    "**The issue is local vs. global mixing.** RWMH with isotropic proposals faces a dilemma on curved distributions like the Rosenbrock:\n",
    "- If the proposal scale is small enough to stay on the narrow ridge, it can't move far along the banana\n",
    "- If the proposal scale is large enough to explore, most proposals step off the ridge and get rejected\n",
    "\n",
    "So RWMH ends up jittering locally — samples decorrelate quickly *within* its local neighborhood, but it never traverses the full banana. Low autocorrelation doesn't mean good exploration!\n",
    "\n",
    "**HMC uses gradients to follow the curve.** It makes long, coherent moves along the ridge without stepping off. The high autocorrelation is just because consecutive samples are along the same trajectory — but they're actually covering the full posterior.\n",
    "\n",
    "This is exactly the kind of geometry where HMC shines, and one of the main motivations for gradient-based samplers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Benchmark Distribution 2: Neal's Funnel\n",
    "\n",
    "Neal's Funnel is a hierarchical distribution that varies dramatically in scale across the space. The narrow \"neck\" of the funnel is notoriously difficult for fixed step-size samplers.\n",
    "\n",
    "$$v \\sim \\mathcal{N}(0, 9), \\quad x \\sim \\mathcal{N}(0, e^v)$$\n",
    "\n",
    "This means:\n",
    "- When $v$ is large and positive, $x$ can vary widely\n",
    "- When $v$ is large and negative, $x$ is tightly constrained near 0\n",
    "- A single step size can't work well everywhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prob_funnel(theta):\n",
    "    \"\"\"Neal's Funnel distribution.\n",
    "\n",
    "    v ~ N(0, 9)\n",
    "    x ~ N(0, exp(v))\n",
    "\n",
    "    This creates a funnel shape where the scale of x depends on v.\n",
    "    The narrow neck (small v) is very hard to sample.\n",
    "    \"\"\"\n",
    "    v, x = theta[0], theta[1]\n",
    "    log_p_v = -0.5 * v**2 / 9  # v ~ N(0, 9)\n",
    "    log_p_x_given_v = -0.5 * x**2 * jnp.exp(-v) - 0.5 * v  # x ~ N(0, exp(v))\n",
    "    return log_p_v + log_p_x_given_v\n",
    "\n",
    "\n",
    "plot_distribution(log_prob_funnel, \"Neal's Funnel\", xlim=(-6, 6), ylim=(-10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run the RWMH and HMC baselines on Neal's Funnel\n",
    "# Use the same workflow as Rosenbrock above — you'll need to tune hyperparameters!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Your Novel Sampler\n",
    "\n",
    "Now it's your turn! Implement your own sampler below. Some ideas:\n",
    "\n",
    "- **Adaptive proposals**: Adjust step size based on acceptance rate or local curvature\n",
    "- **Hybrid methods**: Combine different move types (e.g., local + global moves)\n",
    "- **Modified dynamics**: Change the Hamiltonian, use different integrators, add friction\n",
    "- **Tempering**: Use temperature schedules to help exploration\n",
    "\n",
    "A simple idea with thorough analysis beats a complex idea you don't understand!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test your sampler on both benchmarks (Rosenbrock and Neal's Funnel)\n",
    "# my_samples, my_acc = run_my_sampler(key, log_prob_rosenbrock, initial_pos, n_samples=50_000, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test your sampler on Rosenbrock\n",
    "# my_samples, my_acc = run_my_sampler(key, log_prob_rosenbrock, initial_pos, n_samples=50_000, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare to baselines\n",
    "# - Visualize samples (scatter plot with contours)\n",
    "# - Compute ESS using samples_to_inference_data() and az.summary()\n",
    "# - Plot traces and autocorrelation\n",
    "# - Discuss: where does your method work well? Where does it struggle?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Ablation study\n",
    "# - What are the key hyperparameters of your method?\n",
    "# - How sensitive is performance to each one?\n",
    "# - What happens in limiting cases (e.g., turning off a component)?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
